
\documentclass[12pt]{article}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{chngpage}
\usepackage{soul,color}
\usepackage{hyperref}
\usepackage{graphicx,float,wrapfig}
\usepackage{ listings}
\title{Notes on lower bounding formulas}
% In case you need to adjust margins:
\topmargin=-0.45in      %
\evensidemargin=0in     %
\oddsidemargin=0in      %
\textwidth=6.5in        %
\textheight=9.0in       %
\headsep=0.25in         %


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{q}[theorem]{Question}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{ob}[theorem]{Observation}
\newtheorem{conjecture}[theorem]{Conjecture}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make title


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\.}{,\ldots,}

\begin{document}
		\maketitle 
		
\newcommand{\F}{\mathcal{F}}		
\newcommand{\E}{\mathbb{E}}		
\renewcommand{\a}{\alpha}		

\section{Instance compression for formula lower bounds}

\begin{theorem}[Main Theorem To prove]
For a boolean formula $f$ with $n$ variables, there exists a function $h_f:\{0,1\}^n\rightarrow \{0,1\}^{n^\a}$ where $\a \leq 1$  and a set $A\subset \{0,1\}^{n^\a}$ such that $L(h_f)=o(L(f)^{1-\a})$ and with probability $1/2+\Omega(\delta)$, $$h_f(x)\in A \leftrightarrow f(x)=1.$$
\end{theorem}
Denote $I_A$ is the indicator function for the set $A$, that is $I_A(x)=1\leftrightarrow x\in A$. Then the function $I_A(h_f(\cdot))$ $(1/2+\Omega(\delta))$-agrees with $f(\cdot)$. If $f$ is average hard for $n^3$-sized formula and $k=o(n)$, then $$L(I_A)\geq L(f)/L(h_f)=n^{3\alpha}/o(1)>>n^{3\a}$$. Note that $I_A$ depends on $n^{\a}$ variables, which means the size of $I_A$ is larger than the cube of its size. 


\subsubsection{Negative results for Instance compression}
\begin{theorem}[\cite{fortnow2008infeasibility}]
If there is a set $A$ and function $h$ such
that given $m$ Boolean formula $\phi_1, \ldots, \phi_m$ where each $\phi_i$ has length at most $n$, $h$ has the following
properties:
\begin{enumerate}
\item  $h$ is computable in time polynomial in m and n,
\item  $h(\phi_1, \ldots, \phi_m) \in A$ if and only if at least one of the $\phi_i$ is satisfiable, and
\item  $|h(\phi_1, \ldots, \phi_m)|$ is bounded by a polynomial in n,
\end{enumerate}
then  $NP$ is contained in $coNP/poly$.
\end{theorem}

To apply the above theorem to our setting, we need transfer the satisfiable problem to computable problem... 

\begin{theorem}[Probabilistic version \cite{fortnow2008infeasibility}]
If OR-SAT is probabilistically compressible with randomness complexity $O(log(n))$,
then PH collapses.
\end{theorem}

\section{Generalize the PRG from shrinkage}	

First of all, from the paper \cite{impagliazzo2012pseudorandomness}, we clean out two main lemmas by viewing the restriction as a special function. 
\begin{lemma}\label{main lemma for PRG from shrinkage}
Let $\F:\{0,1\}^n\rightarrow \{0,1\}$ be a class of de Morgan formulas with an associated size function $s:\F \rightarrow \mathbb{N}$. Then, there exists a function $h:\{0,1\}^{s^{2/3}}\rightarrow \{0,1\}^n$ such that $s^{1/3}-$ wise independent input $G_{s^{1/3}}$ could $\epsilon-$fool $f\circ h(\cdot )$, that is 
$$|\E[f\circ h(G_{s^{1/3}})]-\E[f\circ h(U_{s^{2/3}})]|\leq \epsilon.$$
\end{lemma}

\begin{lemma}
For sufficiently large $t$, there exists shrinkage functions $h_1\.h_t$ such that the distribution $\oplus_{i=1}^t f\circ h_i(U_{s^{2/3}})$ is $\epsilon-$close to the distribution $f(U_{n}).$
\end{lemma}

In the Lemma \ref{main lemma for PRG from shrinkage}, view $h(G_{s^{1/3}})$ and $h(U_{s^{2/3}})$ as two different $n$-bits codes $C_1,C_2$. Then Lemma \ref{main lemma for PRG from shrinkage} shows that no $n^3$-sized de Morgan formula $f$ can distinguish $C_1,C_2$ within $\epsilon$.

\begin{q}
For any constant $\epsilon<0$, are there two codes $C_1,C_2\subset \{0,1\}^n$ such that $|C_1|=o(|C_2|)$ and no $n^4$-sized formula $f$ could distinguish them within error $\epsilon$, namely 
$$|E_{x\in C_1}[f(x)]-E_{x\in C_2}[f(x)]|\leq \epsilon$$
\end{q}

\section{Main Goal}
\begin{ob}
	For any formula $f$ on $k$ bits and PRG $G$ whose input is $T$ bits and output is $k$ bits, if $f( G(x|_\rho))=f(U_k)$, then we have 
	$L(f (G))\geq (T/k)^2 L(f)$.
\end{ob}
However, if $L(f)=k^d$, then we can only get $L(f(G))\geq T^2k^{d-2}=O(T^d) $. That is, this method cannot improve our conclusion. But it sheds light on how to composite functions to increase the formula complexity.   Furthermore, the Andreev function is the special case when $k=\log n$ and $T=n$ where $L(f)=\log n$. 


Actually, there has been a similar conjecture: 
\begin{conjecture}[KRW conjecture]
Let $f : \{0, 1\}^n\rightarrow \{0, 1\}$ and $g : \{0, 1\}^m \rightarrow \{0, 1\}$ be non-constant functions. Then
$$D(g \circ f) \simeq D(g) + D(f),$$
where $D(f)$ denotes the depth of $f$ and $g\circ f$ represents the composition of functions, i.e. $g\circ f=g(f(\overline{x_1}),\ldots, f(\overline{x_m}))$, in which $\overline{x_i}$ is the vector of $n$ distinct variables. 
\end{conjecture}

Our first theorem shows that, 
\begin{theorem}
	For any formula $F$ with $k$ variables and  formula $G$ with $T$ variables, if $F\circ G(x|_\rho)=F(U_k)$ and $L(G)\leq T^2$, then the KRW conjecture is true.
\end{theorem}
\begin{q}
	\begin{enumerate}
		\item How about $L(G)\ge T^2$?  Can we develop new technique to prove it, instead of using the Hastad's shrinkage argument?
		\item Maybe the PRG  can have more property, rather than only fixing the bit-fixing source.
		\item Furthermore, to prove a better lower bound, we don't need such strong conjecture. If $F$ has more properties, e.g. $F\in AC^0$ and $F$ is average hardness, we may avoid the difficulty of proving this conjecture. This part is covered in Section \ref{sec:AC0}. I am not sure whether it is  workable? 
		\item What can be revealed for the formula from the random formula restriction?
		\end{enumerate}
		\end{q}
\subsection{Related work}
In the paper \cite{gavinsky2014toward},  Gavinsky, Meir, Weinstein and Avi Wigderson tried to use information-theoretic approach to prove it.  

They have observed the Andreev functions is just 
$L(g \circ \oplus_m) = \Omega( L(g) \cdot L(\oplus_m)).$
Thus, the $F(f_1\.f_n)=t^{d+1}/polylog (t)$ where $t=n\log n$

And they made a new conjecture about $\oplus_m\circ f$, However, this is a significant difference:
when the parity function is “at the bottom” , one can easily apply random restrictions
to the function, but it is not clear how to do it when the parity function is “at the top”.


%\begin{proof}
%First of all, we need th following property:
%\begin{enumerate}
%\item $F(f_1|_\rho\.f_n|_\rho)=F(x_1\.x_n)$
%\end{enumerate}
%That is 
%$$p^2L(F^*)\geq s(n)$$
%If $p=\log n/n$, then we know $L(F^*)\geq n$
%\end{proof}

In Andreev function, we only need $\log n$ variables left but now we need $n$. 


\section{Another Way-- bound the variance of shrinkage}
First of all, we know the shrinkage size $L(\phi|_{\rho})$ is a random variable with expected size $p^2L$ and has a high upper concentration result. We proceed to ask what about $Var[L(\phi|_\rho)]$

We can use some non-malleable code to construct a function with high variance after shrinkage. But it is a bit involved to bound the variance of shrinkage....
\section{Assuming $F$ is AC0 and has average-case hardness }
\label{sec:AC0} 
By \cite{komargodski2013improved}, we know 
\begin{theorem}
There is an explicit Boolean function $h:\{0,1\}^n\rightarrow \{0,1\}$ and a constant $c\geq 8$ and a constant $c\geq 8$ such that for  any parameter $r$ such that $c\log(n)\leq r\le n^{1/3}$, any formula of size $\frac{n^3-o(1)}{r^2}$ computes $h$ correctly on a fraction of at most $1/2+2^{-\Omega(r)}$ of the inputs. 
\end{theorem}

Furthermore, by the \cite{braverman2010polylogarithmic},
\begin{theorem}
Let $s \ge \log m$ be any parameter. Let $F$ be a Boolean function
computed by a circuit of depth $d$ and size $m$. Let $\mu$ be an $r$-independent distribution
where
$$r \geq  r(s, d) = 3 \cdot 60d+3 \cdot (\log m)$$

then
$$|Pr_{x\sim U_{poly\log n}}[f(x=1)]-Pr_{x\sim U_n}[f(x)=1]|=|E_\mu[F] -E[F]| < \epsilon(s, d),$$
where $\epsilon(s, d) = 0.82s \cdot(10m).$
\end{theorem}
\begin{theorem}
$r(m, d,\epsilon)$-independence $\epsilon$-fools depth-$d$ AC0 circuits of size
m, where
$$r(m, d, ε)=\log(\frac{m}{\epsilon}))^{O(d^2)}$$. 
\end{theorem}

Now, we consider the following composition $f\circ G$ on polylog many bits, where $G$ is puesudorandom generator to generate polylog-wise independent distribution from $poly\log n$ inputs. That is, 
$$\big|Pr_{x\sim U_{poly\log n}}[f(x)=1]-Pr_{y\in\{0,1\}^{poly\log n}}[f(G(y))=1]\big|\leq \delta$$
Note that instead of fooling the formula on uniform random distribution, what we want to fool here is the formula over $poly\log n$-wise independent distribution. This should give a exponentially error bound, rather than the polynomially small compared with the uniform distribution case. 


Concluding with the above two theorems, we get 
$$|Pr_{x\sim U_n}[f(x)=1]-Pr_{y\in\{0,1\}^{poly\log n}}[f(G(y))=1]|\leq \delta+\epsilon.$$
where $\epsilon, \delta$ could be both exponentially small. Then we hope to prove the following theorem, and we could get a much better lower bound than $\Omega(n^3)$.

Then, by the paper \cite{trevisan2009regularity}, we want to show the indistinguishability can imply a good approximation for each $f$. If so, we can have the following property. 

\begin{theorem}
	If there are two functions $f:\{0,1\}^n\rightarrow \{0,1\}$ and $g:\{0,1\}^{ n}\rightarrow \{0,1\}$ such that $f$ approximates $g$ within $1/2+\epsilon$, and for any $s$-sized formula $\Phi$, $$Pr_x[f(x)=\Phi(x)]\leq 1/2+\epsilon;$$
	then  we know
	$$L(g)\geq s$$
%If there are two functions $f:\{0,1\}^n\rightarrow \{0,1\}$ and $g:\{0,1\}^{poly\log n}\rightarrow \{0,1\}$ such that $$|E[f(x)]-E[g(x)]|\leq 2^{-\Omega (r)},$$ and we know for any $s$-sized formula $\Phi$, $$Pr_x[f(x)=\Phi(x)]\leq 1/2+2^{\Omega (r)};$$  then we know 
%$$Pr_x[g(x)=\Phi(x)]\leq 1-2^{\Omega(r)},$$
% which means $L(g)\geq s.$
\end{theorem}


%\begin{proof}
%Assume by contradiction, there is a $s$-sized formula $\Phi$ computing $g$. Then based on $\Phi$, we can construct a formula $\Psi$ in the same size computing $f$. 
%
%That is, first of all, divided the input into two parts: $S_1=\{x|\Phi(x)=f(x)\}$ and 
%$S_2=\{x|\Phi(x)\neq f(x)\}$. If there exists a large subset $S^*\subset S_2$ which can be efficiently computed by formula such that $|S_1|+|S^*|> 2^n(1/2+2^{-Omega(r)})$, then it is contradicting with the average-case hardness of $f$. 
%
%First of all, we bound the size of $|S_1|$ by using the first equation. 
%
%Second, we show how to compute the majority of given set efficiently. 
%
%By the above those things, we can get our desired conclusion. 
%\end{proof}

\subsection{Approximate Boolean function by using PRG}
\begin{definition}
For two functions $f, g : \{0, 1\}^l \rightarrow \{0, 1\}$ and a number $0 \le \rho \leq 1$ we say that $g$
approximates $f$ within a factor $\rho$ if $f$ and $g$ agree on at least a fraction $\rho$ of their
domain, i.e.$$ Pr[f(x) = g(x)] \geq  \rho$$
\end{definition}

\begin{theorem}[Main Theorem To prove]
If there is a PRG $G:\{0,1\}^k\rightarrow \{0,1\}^n$ that $\delta$-fools  a class of Boolean functions $C$ with $n$ variables, then for each function $f\in C$, there exists a function $h_f:\{0,1\}^n\rightarrow \{0,1\}^k$  such that $L(h_f)=o(L(f))$ and $f(G(h_f(\cdot )))$ approximates $f$ within $1/2+\Omega(\delta)$.
\end{theorem}

\begin{proof}
That is, we  need to bound 
$$Pr[h_f(x)\in (f(G))^{-1}(1)|f(x)=1]+Pr[h_f(x)\in (f(G))^{-1}(0)|f(x)=0].$$

And what we have is a precise estimate of $E[f(x))]$ by using PRG on $E[f(G(x))]$. Could fourier transform help us here?  Since we know the first coefficient in the fourier transfrom of the function $f$. 
\end{proof}


\bibliographystyle{plain}
\bibliography{Notes}
\end{document} 