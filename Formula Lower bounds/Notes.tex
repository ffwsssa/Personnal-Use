\documentclass[12pt]{article}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{chngpage}
\usepackage{soul,color}
\usepackage{hyperref}
\usepackage{graphicx,float,wrapfig}
\usepackage{ listings}
\title{Notes on lower bounding formulas}
% In case you need to adjust margins:
\topmargin=-0.45in      %
\evensidemargin=0in     %
\oddsidemargin=0in      %
\textwidth=6.5in        %
\textheight=9.0in       %
\headsep=0.25in         %


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{q}[theorem]{Question}
\newtheorem{conjecture}[theorem]{Conjecture}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make title


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\.}{,\ldots,}

\begin{document}
		\maketitle 

\section{Main Goal}
\begin{theorem}
	For any formula $f$ on $k$ bits and PRG $G$ whose input is $T$ bits and output is $k$ bits, if $f( G(x|_\rho))=f(U_k)$, then we have 
	$L(f (G))\geq (T/k)^2 L(f)$.
\end{theorem}
However, if $L(f)=k^d$, then we can only get $L(f(G))\geq T^2k^{d-2}=O(T^d) $. That is, this method cannot improve our conclusion. But it sheds light on how to composite functions to increase the formula complexity.   Furthermore, the Andreev function is the special case when $k=\log n$ and $T=n$ where $L(f)=\log n$. 


Actually, there has been a similar conjecture: 
\begin{conjecture}[KRW conjecture]
Let $f : \{0, 1\}^n\rightarrow \{0, 1\}$ and $g : \{0, 1\}^m \rightarrow \{0, 1\}$ be non-constant functions. Then
$$D(g \circ f) \simeq D(g) + D(f),$$
where $D(f)$ denotes the depth of $f$ and $g\circ f$ represents the composition of functions, i.e. $g\circ f=g(f(\overline{x_1}),\ldots, f(\overline{x_m}))$, in which $\overline{x_i}$ is the vector of $n$ distinct variables. 
\end{conjecture}

Our first theorem shows that, 
\begin{theorem}
	For any formula $F$ with $k$ variables and  formula $G$ with $T$ variables, if $F\circ G(x|_\rho)=F(U_k)$ and $L(G)\leq T^2$, then the KRW conjecture is true.
\end{theorem}
\begin{q}
	\begin{enumerate}
		\item How about $L(G)\ge T^2$?  Can we develop new technique to prove it, instead of using the Hastad's shrinkage argument?
		\item Maybe the PRG  can have more property, rather than only fixing the bit-fixing source.
		\item Also, $F$ could have more properties, e.g. $F\in AC^0$. Then,  this could significantly decrease the of size input(seed) by using a $poly\log n$ independence. This part is covered in Section \ref{sec:AC0} 
		\item What can be revealed for the formula from the random formula restriction?
		\end{enumerate}
		\end{q}
\subsection{Related work}
In the paper \cite{gavinsky2014toward},  Gavinsky, Meir, Weinstein and Avi Wigderson tried to use information-theoretic approach to prove it.  

They have observed the Andreev functions is just 
$L(g \circ \oplus_m) = \Omega( L(g) \cdot L(\oplus_m)).$
Thus, the $F(f_1\.f_n)=t^{d+1}/polylog (t)$ where $t=n\log n$

And they made a new conjecture about $\oplus_m\circ f$, However, this is a significant difference:
when the parity function is “at the bottom” , one can easily apply random restrictions
to the function, but it is not clear how to do it when the parity function is “at the top”.


%\begin{proof}
%First of all, we need th following property:
%\begin{enumerate}
%\item $F(f_1|_\rho\.f_n|_\rho)=F(x_1\.x_n)$
%\end{enumerate}
%That is 
%$$p^2L(F^*)\geq s(n)$$
%If $p=\log n/n$, then we know $L(F^*)\geq n$
%\end{proof}

In Andreev function, we only need $\log n$ variables left but now we need $n$. 


\section{Another Way-- bound the variance of shrinkage}
First of all, we know the shrinkage size $L(\phi|_{\rho})$ is a random variable with expected size $p^2L$ and has a high upper concentration result. We proceed to ask what about $Var[L(\phi|_\rho)]$

\section{Assuming $F$ is AC0 and has average-case hardness }
\label{sec:AC0} 
By \cite{komargodski2013improved}, we know 
\begin{theorem}
There is an explicit Boolean function $h:\{0,1\}^n\rightarrow \{0,1\}$ and a constant $c\geq 8$ and a constant $c\geq 8$ such that for  any parameter $r$ such that $c\log(n)\leq r\le n^{1/3}$, any formula of size $\frac{n^3-o(1)}{r^2}$ computes $h$ correctly on a fraction of at most $1/2+2^{-\Omega(r)}$ of the inputs. 
\end{theorem}

Furthermore, by the \cite{braverman2010polylogarithmic},
\begin{theorem}
Let $s \ge \log m$ be any parameter. Let $F$ be a Boolean function
computed by a circuit of depth $d$ and size $m$. Let $\mu$ be an $r$-independent distribution
where
$$r \geq  r(s, d) = 3 \cdot 60d+3 \cdot (\log m)$$

then
$$|Pr_{x\sim U_{poly\log n}}[f(x=1)]-Pr_{x\sim U_n}[f(x)=1]|=|E_\mu[F] -E[F]| < \epsilon(s, d),$$
where $\epsilon(s, d) = 0.82s \cdot(10m).$
\end{theorem}
\begin{theorem}
$r(m, d,\epsilon)$-independence $\epsilon$-fools depth-$d$ AC0 circuits of size
m, where
$$r(m, d, ε)=\log(\frac{m}{\epsilon}))^{O(d^2)}$$. 
\end{theorem}

Now, we consider the following composition $f\circ G$ on polylog many bits, where $G$ is puesudorandom generator to generate polylog-wise independent distribution from $poly\log n$ inputs. That is, 
$$\big|Pr_{x\sim U_{poly\log n}}[f(x)=1]-Pr_{y\in\{0,1\}^{poly\log n}}[f(G(y))=1]\big|\leq \delta$$
Note that instead of fooling the formula on uniform random distribution, what we want to fool here is the formula over $poly\log n$-wise independent distribution. This should give a exponentially error bound, rather than the polynomially small compared with the uniform distribution case. 


Concluding with the above two theorems, we get 
$$|Pr_{x\sim U_n}[f(x)=1]-Pr_{y\in\{0,1\}^{poly\log n}}[f(G(y))=1]|\leq \delta+\epsilon.$$
where $\epsilon, \delta$ could be both exponentially small. Then we hope to prove the following theorem, and we could get a much better lower bound than $\Omega(n^3)$.

Then, by the paper \cite{trevisan2009regularity}, we want to show the indistinguishability can imply a good approximation for each $f$.

\begin{theorem}[Main theorem to prove]
	If there are two functions $f:\{0,1\}^n\rightarrow \{0,1\}$ and $g:\{0,1\}^{ n}\rightarrow \{0,1\}$ such that $f$ approximates $g$ within $1/2+\epsilon$, and for any $s$-sized formula $\Phi$, $$Pr_x[f(x)=\Phi(x)]\leq 1/2+2^{-\Omega (r)};$$
	then  we know
	$$L(f)\leq L(g)+poly(\log 1/\epsilon)$$
%If there are two functions $f:\{0,1\}^n\rightarrow \{0,1\}$ and $g:\{0,1\}^{poly\log n}\rightarrow \{0,1\}$ such that $$|E[f(x)]-E[g(x)]|\leq 2^{-\Omega (r)},$$ and we know for any $s$-sized formula $\Phi$, $$Pr_x[f(x)=\Phi(x)]\leq 1/2+2^{\Omega (r)};$$  then we know 
%$$Pr_x[g(x)=\Phi(x)]\leq 1-2^{\Omega(r)},$$
% which means $L(g)\geq s.$
 
\end{theorem}
\begin{proof}
Assume by contradiction, there is a $s$-sized formula $\Phi$ computing $g$. Then based on $\Phi$, we can construct a formula $\Psi$ in the same size computing $f$. 

That is, first of all, divided the input into two parts: $S_1=\{x|\Phi(x)=f(x)\}$ and 
$S_2=\{x|\Phi(x)\neq f(x)\}$. If there exists a large subset $S^*\subset S_2$ which can be efficiently computed by formula such that $|S_1|+|S^*|> 2^n(1/2+2^{-Omega(r)})$, then it is contradicting with the average-case hardness of $f$. 

First of all, we bound the size of $|S_1|$ by using the first equation. 

Second, we show how to compute the majority of given set efficiently. 

By the above those things, we can get our desired conclusion. 
\end{proof}
\bibliographystyle{plain}
\bibliography{Notes}
\end{document} 